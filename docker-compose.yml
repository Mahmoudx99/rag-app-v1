services:
  # LLM Service
  llm:
    build:
      context: ./llm_service
      dockerfile: Dockerfile
    container_name: rag_llm
    environment:
      - LLM_PROVIDER=vertex_ai
      - LLM_MODEL=llama-3.1-8b
      - LLM_MAX_TOKENS=4096
      - LLM_TEMPERATURE=0.7
      - VERTEX_AI_PROJECT_ID=${VERTEX_AI_PROJECT_ID:-anb-gpt-prj}
      - VERTEX_AI_LOCATION=${VERTEX_AI_LOCATION:-me-central2}
      - VERTEX_AI_LLM_ENDPOINT_ID=${VERTEX_AI_LLM_ENDPOINT_ID:-your_endpoint_id}
      - GOOGLE_APPLICATION_CREDENTIALS=/app/credentials.json
    volumes:
      - ${GOOGLE_APPLICATION_CREDENTIALS:-./credentials.json}:/app/credentials.json:ro
    ports:
      - "8002:8001"
    networks:
      - rag-network
    restart: on-failure

  # Backend API
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: rag_backend
    environment:
      - DATABASE_URL=sqlite:////data/rag_app.db
      - WATCH_DIR=/data/watch
      - LLM_SERVICE_URL=http://llm:8001
      - GCP_PROJECT_ID=${GCP_PROJECT_ID:-anb-gpt-prj}
      - GCP_REGION=${GCP_REGION:-me-central2}
      - VERTEX_AI_INDEX_ENDPOINT_ID=${VERTEX_AI_INDEX_ENDPOINT_ID}
      - VERTEX_AI_DEPLOYED_INDEX_ID=${VERTEX_AI_DEPLOYED_INDEX_ID}
      - VERTEX_AI_INDEX_ID=${VERTEX_AI_INDEX_ID}
      - GOOGLE_APPLICATION_CREDENTIALS=/app/credentials.json
    volumes:
      # Universal document storage location
      - ./data/watch:/data/watch
      # Shared tracker for deletion synchronization
      - ./data/processed:/data/processed
      # Mount parent data directory for SQLite database
      - ./data:/data
      # GCP credentials for Vertex AI
      - ${GOOGLE_APPLICATION_CREDENTIALS:-./credentials.json}:/app/credentials.json:ro
    ports:
      - "8000:8000"
    depends_on:
      llm:
        condition: service_started
    networks:
      - rag-network
    restart: on-failure

  # Frontend
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    container_name: rag_frontend
    environment:
      - REACT_APP_API_URL=http://localhost:8000/api/v1
    ports:
      - "3000:80"
    depends_on:
      - backend
    networks:
      - rag-network

  # File Watcher Service
  # Monitors a folder for new PDFs and triggers processing automatically
  # GCP Migration: Replace with Cloud Storage triggers + Cloud Functions
  file_watcher:
    build:
      context: ./file_watcher_service
      dockerfile: Dockerfile
    container_name: rag_file_watcher
    environment:
      # Watch folder configuration
      # GCP: This becomes GCS bucket name
      - WATCHER_WATCH_FOLDER=/data/watch

      # Backend API endpoint
      # GCP: This becomes Cloud Run URL or Pub/Sub topic
      - WATCHER_BACKEND_URL=http://backend:8000
      - WATCHER_BACKEND_PROCESS_ENDPOINT=/api/v1/documents/process-file

      # File stability settings (ensures upload is complete)
      # GCP: Not needed with Cloud Storage OBJECT_FINALIZE events
      - WATCHER_FILE_STABILITY_THRESHOLD=5.0
      - WATCHER_FILE_STABILITY_CHECK_INTERVAL=2.0

      # Retry configuration
      - WATCHER_MAX_RETRIES=3
      - WATCHER_RETRY_DELAY=5.0

      # Process existing files on startup
      - WATCHER_PROCESS_EXISTING_ON_STARTUP=true

      # Logging
      - WATCHER_LOG_LEVEL=INFO
    volumes:
      # Watch folder - drop PDFs here for automatic processing
      - ./data/watch:/data/watch
      # Processed file tracker
      - ./data/processed:/data/processed
    depends_on:
      - backend
    networks:
      - rag-network
    restart: on-failure

networks:
  rag-network:
    driver: bridge
